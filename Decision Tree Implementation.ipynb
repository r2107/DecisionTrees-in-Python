{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading IrisDataset and Labelling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find label for a value\n",
    "#if MIN_Value <=val < (m + Mean_Value) / 2 then it is assigned label a\n",
    "#if (m + Mean_Value) <=val < Mean_Value then it is assigned label b\n",
    "#if (Mean_Value) <=val < (Mean_Value + MAX_Value)/2 then it is assigned label c\n",
    "#if (Mean_Value + MAX_Value)/2 <=val <= MAX_Value  then it is assigned label d\n",
    "def label(val, *boundaries):\n",
    "    if val < boundaries[0]:\n",
    "        return 'a'\n",
    "    elif val < boundaries[1]:\n",
    "        return 'b'\n",
    "    elif val < boundaries[2]:\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "#Function to convert a continuous data into labelled data\n",
    "#There are 4 lables  - a, b, c, d\n",
    "def toLabel(df, feat):\n",
    "    second = df[feat].mean()\n",
    "    mini = df[feat].min()\n",
    "    maxi = df[feat].max()\n",
    "    first = (second + mini)/2\n",
    "    third = (second + maxi)/2\n",
    "    return df[feat].apply(label, args = (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sl_label'] = toLabel(df, 'sl')\n",
    "df['sw_label'] = toLabel(df, 'sw')\n",
    "df['pl_label'] = toLabel(df, 'pl')\n",
    "df['pw_label'] = toLabel(df, 'pw')\n",
    "# to drop the previous and extra columns of no use from now\n",
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeNode class for Storing Output of Node and Feature on which it Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data,output):\n",
    "        # data represents the feature upon which the node was split when fitting the training data\n",
    "        # data = None for leaf node\n",
    "        self.data = data\n",
    "        # children of a node are stored as a dicticionary with key being the value of feature upon which the node was split\n",
    "        # and the corresponding value stores the child TreeNode\n",
    "        self.children = {}\n",
    "        # output represents the class with current majority at this instance of the decision tree\n",
    "        self.output = output\n",
    "        # index will be used to assign a unique index to each node\n",
    "        self.index = -1\n",
    "        \n",
    "    def add_child(self,feature_value,obj):\n",
    "        self.children[feature_value] = obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        # root represents the root node of the decision tree built after fitting the training data\n",
    "        self.__root = None\n",
    "               \n",
    "    # calculating entropy\n",
    "    def entropy(self, Y):\n",
    "        # store the total entropy\n",
    "        epy = 0.0 \n",
    "        d = {} # stores the count of all possible values in Y\n",
    "        for i in Y:\n",
    "            if i in d:\n",
    "                d[i] = d[i] + 1\n",
    "            else:\n",
    "                d[i] = 1\n",
    "        total = len(Y)\n",
    "        for i in d:\n",
    "            p = d[i]/total\n",
    "            epy = epy + (((-1.0) * p)*math.log2(p))\n",
    "        return epy\n",
    "\n",
    "    # calculating the gain ratio\n",
    "    def gain(self, X, Y, feat):\n",
    "        info_original = self.entropy(Y)  # current entropy at the noce\n",
    "        info_f = 0.0                  # it tells the entropy after splitting upon the selected feature\n",
    "        original_size = X.shape[0]\n",
    "        unique_values = set(X[feat])\n",
    "        X[X.shape[1]] = Y\n",
    "        split_info = 0.0\n",
    "        for i in unique_values:\n",
    "            new_X = X[X[feat] == i]\n",
    "            newX = new_X.iloc[:,:-1]\n",
    "            newY = new_X[new_X.shape[1]-1]\n",
    "            curr_size = new_X.shape[0]\n",
    "            p = curr_size/original_size\n",
    "            info_f = info_f + (1.0 * p * self.entropy(newY))\n",
    "            split_info = split_info - (1.0 * p * math.log2(p))\n",
    "        info_gain = info_original - info_f\n",
    "        if split_info == 0 :\n",
    "            return math.inf\n",
    "        gain_ratio = info_gain / split_info\n",
    "        return gain_ratio    \n",
    "    \n",
    "    def decisionTree(self, Xtrain, Ytrain, unused_features, level):\n",
    "        # returns the root of the Decision Tree(which consists of TreeNodes) built after fitting the training data\n",
    "        # Here Nodes are printed as in PREORDER traversl\n",
    "        # level represents depth of the tree\n",
    "        # We split a node on a particular feature only once (in a given root to leaf node path)\n",
    "        \n",
    "        # storing the count of the unique values in Ytrain \n",
    "        arr = np.array(Ytrain)\n",
    "        lis = arr.ravel()\n",
    "        uni = {}\n",
    "        for i in range(len(lis)):\n",
    "            if lis[i] not in uni:\n",
    "                uni[lis[i]] = 1\n",
    "            else:\n",
    "                uni[lis[i]] = uni[lis[i]] + 1\n",
    "\n",
    "        # output will store the maxing count from each of the possible values in Y\n",
    "                \n",
    "        # Base Case 1\n",
    "        # pure node\n",
    "        if len(uni) == 1:\n",
    "            output = None\n",
    "            print(\"Level \", level)\n",
    "            print(\"Pure Node\")\n",
    "            print(\"Current Entropy is 0.0\")\n",
    "            if 0 in uni:\n",
    "                output = 0\n",
    "                print(\"Count of 0 is\", uni[0])\n",
    "            elif 1 in uni:\n",
    "                output = 1\n",
    "                print(\"Count of 1 is\", uni[1])\n",
    "            else:\n",
    "                output = 2\n",
    "                print(\"Count of 2 is\", uni[2])\n",
    "            print()\n",
    "            return TreeNode(None, output)\n",
    "\n",
    "        # Base Case 2\n",
    "        # length of features is zero\n",
    "        if(len(unused_features) == 0):\n",
    "            print(\"Level \", level)\n",
    "            output = None\n",
    "            max_count = -1000\n",
    "            if 0 in uni:\n",
    "                if max_count < uni[0]:\n",
    "                    max_count = uni[0]\n",
    "                    output = 0\n",
    "                print(\"Count of 0 is\", uni[0])\n",
    "            else:\n",
    "                print(\"Count of 0 is\", 0)\n",
    "            if 1 in uni:\n",
    "                if max_count < uni[1]:\n",
    "                    max_count = uni[1]\n",
    "                    output = 1\n",
    "                print(\"Count of 1 is\", uni[1])\n",
    "            else:\n",
    "                print(\"Count of 1 is\", 0)\n",
    "            if 2 in uni:\n",
    "                if max_count < uni[2]:\n",
    "                    max_count = uni[2]\n",
    "                    output = 2\n",
    "                print(\"Count of 2 is\", uni[2])\n",
    "            else:\n",
    "                print(\"Count of 2 is\", 0)\n",
    "            print(\"Current Entropy is \", self.entropy(Ytrain))\n",
    "            print(\"Reached Leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None, output)\n",
    "\n",
    "        maxgain = -math.inf    # to store the max gain till then\n",
    "        best_feature = None    # to store the best feature\n",
    "        for f in unused_features:\n",
    "            g = self.gain(Xtrain, Ytrain, f)\n",
    "            if(g > maxgain):\n",
    "                maxgain = g\n",
    "                best_feature = f\n",
    "                \n",
    "        # printing the current Node and calculating output to store in NODE\n",
    "        print(\"Level \", level)\n",
    "        print(unused_features)\n",
    "        output = None\n",
    "        max_count = -1000\n",
    "        if 0 in uni:\n",
    "            if max_count < uni[0]:\n",
    "                max_count = uni[0]\n",
    "                output = 0\n",
    "            print(\"Count of 0 is\", uni[0])\n",
    "        else:\n",
    "            print(\"Count of 0 is\", 0)\n",
    "        if 1 in uni:\n",
    "            if max_count < uni[1]:\n",
    "                max_count = uni[1]\n",
    "                output = 1\n",
    "            print(\"Count of 1 is\", uni[1])\n",
    "        else:\n",
    "            print(\"Count of 1 is\", 0)\n",
    "        if 2 in uni:\n",
    "            if max_count < uni[2]:\n",
    "                max_count = uni[2]\n",
    "                output = 2\n",
    "            print(\"Count of 2 is\", uni[2])\n",
    "        else:\n",
    "            print(\"Count of 2 is\", 0)\n",
    "        print(\"Current Entropy is \", self.entropy(Ytrain))\n",
    "        print(\"Split on Feature \",best_feature,\" with Gain Ratio \",maxgain)\n",
    "        print()\n",
    "\n",
    "        # creating current node with output\n",
    "        current_node = TreeNode(best_feature,output)\n",
    "\n",
    "        unique_values = set(Xtrain[best_feature])\n",
    "        unused_features.remove(best_feature)\n",
    "        level = level + 1\n",
    "        Xtrain[Xtrain.shape[1]] = Ytrain\n",
    "        \n",
    "        # recursively calling on the splits and storing each child of current node in it\n",
    "        for i in unique_values:\n",
    "            new_Z = Xtrain[Xtrain[best_feature] == i]\n",
    "            X = new_Z.iloc[:,:-1]\n",
    "            Y = new_Z[new_Z.shape[1] - 1]\n",
    "            node = self.decisionTree(X, Y, unused_features, level)\n",
    "            current_node.add_child(i,node)\n",
    "\n",
    "        unused_features.add(best_feature)\n",
    "        return current_node\n",
    "    \n",
    "    # fitting and starting our Decision Tree\n",
    "    def fit(self, X, Y, ft):\n",
    "        self.root = self.decisionTree(X, Y, ft, 0)\n",
    "    \n",
    "    # predicting output for each row\n",
    "    def predIn(self, dat, node):\n",
    "        # if the node is leaf node\n",
    "        if len(node.children) == 0 :\n",
    "            return node.output\n",
    "\n",
    "        val = dat[node.data] # represents the value of feature on which the split was made       \n",
    "        if val not in node.children :\n",
    "            return node.output\n",
    "\n",
    "        # Recursively call on the splits\n",
    "        return self.predIn(dat, node.children[val])\n",
    "\n",
    "    # predicting the outputs of X and returning them\n",
    "    def pred(self, X):\n",
    "        y = np.array([0 for i in range(len(X))])\n",
    "        for i in range(len(X)):\n",
    "            y[i] = self.predIn(X.iloc[i,:], self.root)\n",
    "        return y\n",
    "\n",
    "    # to calculate the score and return the predictions and score\n",
    "    def score(self, X, Y):\n",
    "        Y_pred = self.pred(X)\n",
    "        count = 0      # count will store the number of truth predictions\n",
    "        for i in range(len(X)):\n",
    "            if Y_pred[i] == Y[i]:\n",
    "                count = count + 1\n",
    "        return Y_pred, count/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data and Printing the each Node in Different Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "{'pw_label', 'pl_label', 'sl_label', 'sw_label'}\n",
      "Count of 0 is 50\n",
      "Count of 1 is 50\n",
      "Count of 2 is 50\n",
      "Current Entropy is  0.04819212460330587\n",
      "Split on Feature  pw_label  with Gain Ratio  -0.15183643117217138\n",
      "\n",
      "Level  1\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 2 is 34\n",
      "\n",
      "Level  1\n",
      "{'pl_label', 'sl_label', 'sw_label'}\n",
      "Count of 0 is 0\n",
      "Count of 1 is 40\n",
      "Count of 2 is 16\n",
      "Current Entropy is  0.863120568566631\n",
      "Split on Feature  pl_label  with Gain Ratio  0.4334099495621067\n",
      "\n",
      "Level  2\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 2 is 8\n",
      "\n",
      "Level  2\n",
      "{'sl_label', 'sw_label'}\n",
      "Count of 0 is 0\n",
      "Count of 1 is 39\n",
      "Count of 2 is 8\n",
      "Current Entropy is  0.6581912658132185\n",
      "Split on Feature  sl_label  with Gain Ratio  0.12674503775809332\n",
      "\n",
      "Level  3\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 1 is 2\n",
      "\n",
      "Level  3\n",
      "{'sw_label'}\n",
      "Count of 0 is 0\n",
      "Count of 1 is 23\n",
      "Count of 2 is 7\n",
      "Current Entropy is  0.783776947484701\n",
      "Split on Feature  sw_label  with Gain Ratio  0.07092036405148876\n",
      "\n",
      "Level  4\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 1 is 6\n",
      "\n",
      "Level  4\n",
      "Count of 0 is 0\n",
      "Count of 1 is 3\n",
      "Count of 2 is 1\n",
      "Current Entropy is  0.8112781244591328\n",
      "Reached Leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of 0 is 0\n",
      "Count of 1 is 14\n",
      "Count of 2 is 6\n",
      "Current Entropy is  0.8812908992306927\n",
      "Reached Leaf Node\n",
      "\n",
      "Level  3\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 2 is 1\n",
      "\n",
      "Level  3\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 1 is 14\n",
      "\n",
      "Level  2\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 1 is 1\n",
      "\n",
      "Level  1\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 0 is 50\n",
      "\n",
      "Level  1\n",
      "Pure Node\n",
      "Current Entropy is 0.0\n",
      "Count of 1 is 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTreeClassifier()     # creating object of our Decision Tree\n",
    "y = pd.DataFrame(iris.target)       # y contains the target values of our training data\n",
    "unused_features = set(df.columns)   # unused_features contains the features names of our trainig data\n",
    "raw = df.copy()                     # saving a copy of Data to work again on it\n",
    "yraw = y.copy()                     # saving a copy of Data to work again on it\n",
    "clf1.fit(raw,yraw,unused_features)  # fitting data to our Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Predictions and Score of our Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 2 1]\n",
      "Score : 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# converting the 2D dataFrame to 1D array\n",
    "arr = np.array(y)                   \n",
    "lis = arr.ravel()\n",
    "\n",
    "# checking our score and predictions\n",
    "our_predictions, our_score = clf1.score(df, lis)\n",
    "print(\"Predictions\", our_predictions)              # predictions of our Decision Tree\n",
    "print(\"Score :\", our_score)                    # score of our Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data in inbulit Decision Tree and printing Predictions and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 1 2 2 2 2 2 1 2 2 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 1 2\n",
      " 2 1]\n",
      "Score : 0.92\n"
     ]
    }
   ],
   "source": [
    "# Using the inbuilt sklearn Decision tree and comparing it with our model on the same dataset\n",
    "import sklearn.tree\n",
    "clf3 = sklearn.tree.DecisionTreeClassifier()\n",
    "\n",
    "# to convert the strings a, b, c, d to integer values\n",
    "for i in range(len(raw)):\n",
    "    for j in range(len(raw.iloc[i,:])):\n",
    "        z = raw.iloc[i,j]\n",
    "        o = None\n",
    "        if z == 'a':\n",
    "            o = 1\n",
    "        elif z == 'a':\n",
    "            o = 2\n",
    "        elif z == 'c':\n",
    "            o = 3\n",
    "        else:\n",
    "            o = 4\n",
    "        raw.iloc[i,j] = o\n",
    "\n",
    "\n",
    "clf3.fit(raw, yraw)                  # fit the data to inbulit tree\n",
    "Y_pred3 = clf3.predict(raw)         # predicting the results from inbuilt tree\n",
    "print(\"Predictions\", Y_pred3)        # printing the predictions from inbuilt tree\n",
    "sklearn_score = clf3.score(raw, yraw)  \n",
    "print(\"Score :\", sklearn_score)      # score of the inbuilt tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
